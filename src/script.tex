\documentclass[a4paper]{article}

\usepackage{verbatim}
\input{../latex-std/lang-de.tex}


\usepackage[affil-it]{authblk}

%%%%%%% bibliography. biber
\usepackage{microtype} % get rid of bad boxes (overful hbox) in bibliography {https://www.mrunix.de/forums/showthread.php?76019-Biblatex-Overfull-Boxes-im-Literaturverzeichnis-beheben-kein-minimal-bsp}

\usepackage{csquotes} % {"When using babel or polyglossia with biblatex, loading csquotes is recommended to ensure that quoted texts are typeset according to the rules of your main language."} {https://tex.stackexchange.com/questions/229638/package-biblatex-warning-babel-polyglossia-detected-but-csquotes-missing/229653}
\usepackage[backend=biber]{biblatex}
\addbibresource{bibliography.bib}
\usepackage{bbold}
\usepackage[noend]{algpseudocode}
\usepackage{multicol}
\usepackage[official]{eurosym} % € - Symbol
\newcommand{\mc}{Markow-Kette}
\title{Effiziente Berechnung von Varianzen in \mc{}n}%und stabile
\author{Maximilian Starke}
\affil{Fakultät für Informatik, Technische Universität Dresden}
\date{\today}
\usepackage{mathtools}
\usepackage{ragged2e}
\usepackage{framed}
\usepackage{amsmath, amssymb}
\usepackage{enumerate}
\usepackage{tabularx}
\DeclareMathOperator*{\argmin}{\arg\min}
\usepackage{pgfplots}
\pgfplotsset{width=10cm,compat=1.10}
\usepgfplotslibrary{fillbetween}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\usepackage{listings}
\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}
\newcolumntype{L}[1]{>{\flushleft\arraybackslash}m{#1}}
\usepackage{tikz}
\usepackage{verbatim}
\usetikzlibrary{%
	arrows,
	shapes,
	shapes.misc,% wg. rounded rectangle
	shapes.arrows,%
	chains,%
	matrix,%
	positioning,% wg. " of "
	backgrounds,
	fit,
	petri,
	scopes,%
	decorations.pathmorphing,% /pgf/decoration/random steps | erste Graphik
	shadows,%
	calc
}
%#1
\tikzstyle{vertex}=[circle, minimum size=20pt, line width = 1pt, draw = black]
\tikzstyle{target} = [vertex, double, double distance = 1pt]
\tikzstyle{edge} = [draw,shorten > = 1pt, shorten < = 1pt, line width=1pt,->]
\tikzstyle{medge} = [draw, line width = 8pt, yellow!50]
\tikzstyle{weight} = [font=\small]
\tikzstyle{selected edge} = [draw,line width=5pt,-,red!50]
\tikzstyle{ignored edge} = [draw,line width=5pt,-,black!20]

\usepackage{relsize}

\usepackage{xcolor}
% maybe install minted some day and make syntax highlighting###

\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage{amsmath, amssymb}
\usepackage{enumerate}
\usepackage{multicol} % multiple collums in enumerate

\usepackage[thmmarks,amsmath,hyperref,noconfig]{ntheorem} 
% erlaubt es, Sätze, Definitionen etc. einfach durchzunummerieren.
\newtheorem{satz}{Satz}[section] % Nummerierung nach Abschnitten
\newtheorem{proposition}[satz]{Proposition}
\newtheorem{korollar}[satz]{Korollar}
\newtheorem{lemma}[satz]{Lemma}
\newtheorem{vermutung}[satz]{Vermutung}

\theorembodyfont{\upshape}
\newtheorem{beispiel}[satz]{Beispiel}
\newtheorem{bemerkung}[satz]{Bemerkung}
\newtheorem{definition}[satz]{Definition} %[section]
\newtheorem{algorithmus}[satz]{Algorithmus}

\theoremstyle{nonumberplain}
\theoremheaderfont{\itshape}
\theorembodyfont{\normalfont}
\theoremseparator{.}
\theoremsymbol{\ensuremath{_\Box}}
\newtheorem{beweis}{Beweis}
\newtheorem{beweiss}{Beweisskizze}

\qedsymbol{\ensuremath{_\Box}}

\usepackage{chngcntr}
\counterwithin{figure}{section}

\tikzstyle{block} = [rectangle, draw, fill=blue!40, 
text width=7em, text centered, rounded corners, minimum height=5em, node distance= 4.5cm, line width = 2pt]


\tikzstyle{cblock} = [rectangle, draw, fill=blue!40, 
text width=7em, text centered, rounded corners, minimum height=5em, node distance= 3.0cm, line width = 2pt]


\tikzstyle{line} = [draw, -latex', line width = 1pt]


\tikzstyle{cloud} = [ fill = white, rectangle, draw, rounded corners, node distance=2cm,
minimum height=2.5em]

\pgfdeclarelayer{bg}
%\pgfsetlayers{bg,main}	

\pgfdeclarelayer{foreground}
\pgfdeclarelayer{background}
% tell TikZ how to stack them (back to front)
\pgfsetlayers{bg,background,main,foreground}

\newenvironment{meta}
{\begin{center} \Large \color{red} META: \hspace{2ex} \large \color{blue}}
	{\end{center}}

\begin{document}

	\section{Einführung}
	
	Zur Analyse relevanter Zielgrößen in probabilistischen Systemen sind \mc{}n mit Kantengewichten ein wesentliches, häufig genutztes Modell. Mit dessen Hilfe kann beispielsweise die Dauer eines Verbindungsaufbaus in Netzwerken modelliert werden. Wir wollen in dieser Arbeit Varianzen akkumulierter Kantengewichte auf endlichen Pfaden in \mc{}n bis zum Erreichen einer Menge von Zielzuständen und deren effiziente Berechnung betrachten.
	Während Erwartungswerte der akkumulierten Kantengewichte in der wissenschaftlichen Literatur bereits ausführlich untersucht worden sind, trifft dies nicht auf die Varianzen zu, obgleich diese besonders in sicherheitskritischen Systemen oder zur Beurteilung von Risiken durch Abweichung von der Erwartung besondere Relevanz erhalten.
	Verhoeff \cite{Verh04} trug praktische Anwendungen zusammen, von denen wir hier zwei erläutern wollen:
	
	Die niederländische Regierung entschied einst in Bezug auf Probleme mit hohem Verkehrsaufkommen, anstatt auf möglichst geringe Erwartungswerte der Fahrtzeit hinzuwirken, eher die Minimierung der Varianz der Fahrtzeit in den Fokus zu nehmen. Auf diese Weise ist man im Durchschnitt zwar länger unterwegs, erreicht aber mehr Planungssicherheit, kann also Ankunftszeiten genauer vorhersagen:
	Wenn wir mit einer gewissen Wahrscheinlichkeit $\geq p$ rechtzeitig an einem Ort ankommen möchten, dann subtrahieren wir von der gewünschten Ankunftszeit den Erwartungswert der Fahrtdauer und einen Zeitpuffer, welcher wesentlich von der Varianz abhängt.
	
	In der Wirtschaft kommt es immer wieder vor, dass Budgets für bestimmte Ausgaben geplant werden. Insofern größere Abweichungen der Kosten nach oben unbedingt zu vermeiden sind, kann es von Vorteil sein, anstatt eine Investition in Höhe von fiktiven 1000\euro{} zu planen, bei der mit 300\euro{} Abweichung der tatsächlichen Kosten gerechnet werden muss, eine alternative Investition zum selben Zweck .in Höhe von erwarteten 1100\euro{} anzustreben, bei welcher eine Abweichung von nur 50\euro{} erwartet wird.
	
	
	Verhoeff präsentierte lineare Gleichungssysteme für die Berechnung von Varianzen und Kovarianzen \cite{Verh04}. Wir werden zunächst die erforderlichen Grundlagen zu Wahrscheinlichkeitstheorie sowie \mc{}n für den Leser darlegen. Anschließend werden wir einen Algorithmus zur Berechnung von Varianzen und Kovarianzen akkumulierter Kantengewichte formal herleiten. Wir werden  die Betrachtungen von Verhoeff insbesondere um eine ausführliche Herleitung für die Berechnung von Kovarianzen erweitern und zeigen, dass alle ermittelten Gleichungssysteme tatsächlich eindeutig lösbar sind. Danach werden wir eine Implementation des hergeleiteten Algorithmus' hinsichtlich ihrer Performance analysieren. Zum Abschluss werden wir noch einmal den Blick auf Markow-Entscheidungsprozesse lenken und uns der Problemstellung widmen, wie Varianzen minimiert werden können.
	
	\section{Grundlagen}
	
	\subsection{Wahrscheinlichkeitstheorie}
	
	\newcommand{\probspace}{diskreter Wahr\-schein\-lich\-keits\-raum}
	\newcommand{\probspacen}{diskreten Wahr\-schein\-lich\-keits\-raum}
	\newcommand{\probspaceexraw}{(\Omega, P)}
	\newcommand{\probspaceex}{$(\Omega, P)$}
	\begin{definition}[\probspace{}] \label{def-probspace}
		\hspace{-0.5em} Wir nennen ein Paar \probspaceex{} einen \probspacen{}, wenn $\Omega$ eine abzählbare Menge ist und $P : \Omega \to [0,1] $ eine Funktion mit
		\begin{equation}
		\sum_{\omega \in \Omega} P(\omega) = 1 \text{.}
		\end{equation} Wir nennen $\Omega$ in diesem Zusammenhang auch Ergebnismenge und $P$ eine Wahr\-schein\-lich\-keitsverteilung auf $\Omega$.
	\end{definition}
	\newcommand{\rvar}{Zufallsvariable}
	\begin{definition}[\rvar{}] \label{def-rvar}
		Sei \probspaceex{} ein \probspace{}. Eine Funktion $X : \Omega \to \mathbb{R}$ heißt \rvar{} auf \probspaceex{}. Wir nennen $X$ auch kurz \rvar{}, falls der Kontext \probspaceex{} klar ist.
	\end{definition}
	\newcommand{\expect}{Erwartungswert}
	\newcommand{\mexp}{\mathcal{E}}
	\begin{definition}[\expect{}] \label{def-expect}
		\hspace{1ex} Sei \probspaceex{} ein \probspace{} und $X$ eine \rvar{} auf \probspaceex{}. Mit
		\begin{equation}
		\mathcal{E}_{\probspaceexraw{}}(X) \coloneqq \sum_{\omega \in \Omega}{P(\omega) \cdot X(\omega)}
		\end{equation}
		bezeichnen wir den \expect{} von $X$ auf \probspaceex{}. Sollte der Kontext \probspaceex{} klar sein, schreiben wir auch kurz $\mathcal{E}(X)$. Es sei angemerkt, dass es für $|\Omega|=\infty$ Fälle gibt, bei denen die Summe nicht beschränkt ist. Wir schreiben dafür $\mathcal{E}(X) = \infty$ bzw. $\mathcal{E}(X) = -\infty$ und sagen, dass der Erwartungswert nicht existiert.
	\end{definition}
	\begin{beispiel}
		Seien $\Omega = \mathbb{N}_{>0}$, $P : \Omega \to [0,1] : n \mapsto 2^{-n}$ und $R : \Omega \to \mathbb{R} : n \mapsto 2^{n}$. Dann gibt es offensichtlich für jede Zahl $a \in \mathbb{R}$ eine endliche Teilmenge von $T \subseteq \Omega$ mit $\sum_{n \in T}{P(n) \cdot R(n)} \geq a$. Dazu wählen wir einfach $T \coloneqq \{n \in \mathbb{N}_{>0}\mid n \leq \lceil a \rceil \}$ und erhalten $\sum_{n \in T}{P(n) \cdot R(n)} = |T| = \lceil a \rceil \leq \mathcal{E}(R)$. Damit erhalten wir $\mathcal{E}(R) = \infty$.
	\end{beispiel}
	Wir werden uns im Folgenden nur auf \rvar n beziehen, für welche ein Erwartungswert existiert.
	\begin{beispiel}\label{example-expect}
		Ein manipulierter Spielwürfel habe 6 Seiten $\Omega = \{1,2,3,4,5,6\}$, wobei die $1$ im Vergleich zu einem herkömmlichen Würfel mit einer $2$ überklebt wurde. Dies drücken wir durch die Zufallsvariable $X : \Omega \to \mathbb{R} : s \mapsto \max(2,s)$ aus. Außerdem wurde der Würfel mit ungleichmäßig verteilter Masse so gefertigt, dass nach einem Wurf nicht alle Seiten gleich wahrscheinlich oben liegen. Wir nehmen für dieses Beispiel eine Wahrscheinlichkeitsverteilung mit $P(1)=0,1$, $P(2)=0,15$, $P(3)=0,15$, $P(4)=0,15$, $P(5)=0,15$, $P(6)=0,3$ an.
		Dann ist eine Augenzahl pro Wurf von
		\[
		\mathcal{E}(X) = (0,1 + 0,15) \cdot 2 + 0,15 \cdot (3 + 4 + 5) + 0,3 \cdot 6 = 4,1
		\]
		zu erwarten und damit $0,6$ Augen mehr als bei einem ungezinkten Würfel mit \expect{} $3,5$. Würden wir nun alle beschrifteten Zahlen noch zusätzlich quadrieren, erhielten wir einen Würfel mit $Y : \Omega \to \mathbb{R} : s \mapsto \max(4, s^2)$ und würden im Schnitt $19,3$ Augen bei einem Wurf erwarten:
		\[
		\mathcal{E}(Y) = (0,25) \cdot 4 + 0,15 \cdot (9 + 16 + 25) + 0,3 \cdot 36 = 19,3
		\]
	\end{beispiel}
	
	Es lässt sich leicht die folgende Linearität des \expect{}es beobachten:
	
	\begin{lemma} \label{lem-explin}
		Seien $X$, $Y$ Zufallsvariablen auf demselben \probspace{}, $c,d \in \mathbb{R}$. Dann gilt:
		\begin{equation}
		\mathcal{E}(X + cY + d) = \mathcal{E}(X) + c \mathcal{E}(Y) + d \label{eq-linearity}
		\end{equation}
		Dabei ist  $X + cY + d$ die übliche Notation für die \rvar{} gegeben durch $Z : \Omega \to \mathbb{R} : \omega \mapsto X(\omega) + c \cdot Y(\omega)+ d$.
	\end{lemma}
	\begin{beweis}
		\begin{align}
		\mathcal{E}(X + cY + d) & = \sum_{\omega \in \Omega}{P(\omega) \cdot (X + cY + d)(\omega) } \nonumber \\
		& = \sum_{\omega \in \Omega}{P(\omega) \cdot X(\omega) + c \cdot P(\omega) \cdot Y(\omega) + d \cdot P(\omega)} \nonumber \\
		& = \mathcal{E}(X) + c \mathcal{E}(Y) + d \nonumber
		\end{align}
	\end{beweis}
	\newcommand{\var}{Varianz}
	\newcommand{\mvar}{\mathcal{V}\!ar}
	\begin{definition}[\var]\label{def-var}
		Sei \probspaceex{} ein \probspace{} und $X$ eine \rvar{} auf \probspaceex{}. Mit
		\begin{equation}
		\mvar_{\probspaceexraw{}}(X) \coloneqq  \mathcal{E}_{\probspaceexraw{}}\left(\left(X - \mathcal{E}_{\probspaceexraw{}} (X)\right)^{2}\right)
		\end{equation}
		bezeichnen wir die \var{} von $X$ auf  \probspaceex{}. Sollte der Kontext \probspaceex{} klar sein, schreiben wir kurz $\mvar(X)$.
	\end{definition}
	\newcommand{\cov}{Kovarianz}
	\newcommand{\mcov}{\mathcal{C}\!ov}
	\begin{definition}[\cov]\label{def-cov}
		Seien \probspaceex{} ein \probspace{} und $X, Y$ \rvar n auf \probspaceex{}. Die \cov{} $\mcov{}_{\probspaceexraw}(X,Y)$ ist definiert durch:
		\begin{equation}
		\mcov{}_{\probspaceexraw}(X,Y) \coloneqq \mathcal{E} \Big( \big(X - \mathcal{E}(X)\big)\big(Y - \mathcal{E}(Y) \big)\Big)
		\end{equation}
	\end{definition}
	
	Offensichtlich ist die \var{} ein Spezialfall der \cov{}, denn aus den Definitionen folgt unmittelbar $\mvar_{\probspaceexraw}(X) = \mcov{}_{\probspaceexraw}(X,X)$. Wir betrachten im Folgenden, wie sich Kovarianzen durch Erwartungswerte beschreiben lassen.
	
	\begin{lemma}\label{lemma-cov-exp}
		Seien $X$ und $Y$ \rvar{}n auf demselben \probspace{}. Dann gilt:
		\begin{equation}
		\mcov{}(X,Y) = \mathcal{E}(XY) - \mathcal{E}(X)\mathcal{E}(Y)
		\end{equation}
	\end{lemma}
	\begin{beweis}
		\begin{align*}
		\mcov{}(X,Y) & = \mathcal{E}\big( \left(X - \mathcal{E}(X)\right)\left(Y - \mathcal{E}(Y)\right)\big) && \text{(Definition \ref{def-cov})} \\
		& = \mathcal{E}\big(XY - X \mathcal{E}(Y) - Y \mathcal{E}(X) + \mathcal{E}(X)\mathcal{E}(Y)\big) \\
		& = \mathcal{E}(XY) - 2 \mathcal{E}(X) \mathcal{E}(Y) + \mathcal{E}(X) \mathcal{E}(Y) && \text{(Lemma \ref{lem-explin})}\\
		& = \mathcal{E}(XY) - \mathcal{E}(X)\mathcal{E}(Y) \\
		\end{align*}
	\end{beweis}
	
	Betrachten wir den Spezialfall $X = Y$  von Lemma \ref{lemma-cov-exp}, dann erhalten wir folgende Beziehung für \var{}en:
	
	\begin{korollar}\label{kor-var-exp}
		Sei $X$ eine \rvar{} auf einem \probspace{}. Dann gilt:
		\begin{equation}
		\mvar(X) = \mathcal{E}(X^{2}) - \mathcal{E}\left(X\right)^{2}
		\end{equation}
	\end{korollar}
	%\begin{beweis}
	%	\begin{align*}
	%		\mvar(X) & = \mathcal{E}\left( \left(X - \mathcal{E}(X)\right)^{2}\right) && \text{(Definition \ref{def-var})} \\
	%		& = \mathcal{E}(X^{2} - 2 X \mathcal{E}(X) + \mathcal{E}(X)^{2}) \\
	%		& = \mathcal{E}(X^2) - 2 \mathcal{E}(X) \mathcal{E}(X) + \mathcal{E}(X)^{2} && \text{(Linearität (\ref{eq-linearity}))}\\
	%		& = \mathcal{E}(X^{2}) - \mathcal{E}\left(X\right)^{2} \\
	% 	\end{align*}
	%\end{beweis}
	Ähnlich zur Linearität von \expect{}en (\ref{eq-linearity}) lässt sich für \var{}en folgende Beziehung feststellen:
	\begin{lemma}\label{lemma-var-qlinear}
		Sei $X$ eine \rvar{} auf einem \probspace{}, $c,d \in \mathbb{R}$. Dann gilt:
		\begin{equation}
		\mvar(cX + d) = c^2\cdot\mvar(X)
		\end{equation}
	\end{lemma}
	\begin{beweis}
		\begin{align*}
		\mvar(cX + d) & = \mathcal{E}\Big(\big(cX + d - \mathcal{E}(cX + d)\big)^2\Big) && \text{(Defintion \ref{def-var})} \\
		& = \mathcal{E}\Big(\big(cX + d - c\mathcal{E}(X) - d\big)^2\Big) && \text{(Lemma \ref{lem-explin}}\\
		& = \mathcal{E}\Big(c^2 \cdot \big(X - \mathcal{E}(X) \big)^2\Big)\\
		& = c^2 \cdot \mathcal{E}\Big( \big(X - \mathcal{E}(X) \big)^2\Big) && \text{(Lemma \ref{lem-explin}}\\
		& = c^2 \cdot \mvar(X) && \text{(Defintion \ref{def-var})}
		\end{align*}
	\end{beweis}
	Entsprechend der Intuition ist die Varianz, das Quadrat der Standardabweichung, invariant unter Addition einer Konstanten zur \rvar{}. Wird jedoch die \rvar{} mit einem Faktor skaliert, so ändert sich die Varianz um das Quadrat des Faktors. Aus gutem Grund wird die Varianz auch mittlere quadratische Abweichung genannt.
	
	
	Durch das Wegfallen von $d$ lässt sich recht trivial auf Gleichungen schließen, welche unter Umständen nicht mehr auf dem ersten Blick als klar und offensichtlich angesehen werden können. Das folgende Korollar soll dies verdeutlichen:
	\begin{korollar}
		Sei $X$ eine Zufallsvariable und $c \in \mathbb{R}$. Dann gilt:
		\begin{equation}
		\mathcal{E}\left((c+X)^2\right) = (c+ \mathcal{E}(X))^2 + \mvar(X)
		\end{equation}
	\end{korollar}
	\begin{beweis}
		Aus Korollar \ref{kor-var-exp} folgt unmittelbar \[\mvar(X+c) = \mathcal{E}\left((X+c)^{2}\right) - \left(\mathcal{E}\left(X+c\right)\right)^{2}\text{.}\] Die linke Seite kann nach Lemma \ref{lemma-var-qlinear} vereinfacht werden zu $\mvar(X+c) = \mvar(X)$. Der Subtrahend $\left(\mathcal{E}\left(X+c\right)\right)^{2}$ lässt sich aufgrund der Linearität (Lemma \ref{lem-explin}) auch als $\left( \mathcal{E}(X)+c\right)^{2}$ 
		schreiben.
	\end{beweis}
	
	\begin{beispiel}
		Wir wollen nun Beispiel \ref{example-expect} fortsetzen. 
		Nach Korollar \ref{kor-var-exp} beträgt die \var{} beim ursprünglichen Würfel also $\mvar(X) = \mathcal{E}(X^2) - \mathcal{E}(X)^2 = \mathcal{E}(Y) - \mathcal{E}(X)^2 = 19,3 - 16,81 = 2,49$. Es sei bemerkt, dass wir $Y$ geschickt so gewählt haben, dass $Y = X^2$ gilt. Und tatsächlich ergibt die Berechnung nach Definition \ref{def-var} denselben Wert:
		\begin{align*}
		\mvar(X) & = 0,15 \cdot \big((3- 4,1)^2 + (4-4,1)^2 + (5-4,1)^2\big) + && \\
		& \hspace{1.2em} + 0,25 \cdot (2 - 4,1)^2 + 0,3 \cdot (6-4,1)^2 && \\
		& = 0,25 \cdot 4,41 + 0,15 \cdot \big(1,21 + 0,01 + 0,81\big) + 0,3 \cdot 3,61 && \\
		& = 2,49 &&
		\end{align*}
		Die \cov{} lässt sich nach Lemma \ref{lemma-cov-exp} berechnen als $\mcov(X,Y) = \mathcal{E}(XY) - \mathcal{E}(X)\mathcal{E}(Y)$.
		Wir kennen bereits $\mathcal{E}(X)$ sowie $\mathcal{E}(Y)$ und es gilt:
		\[
		\mexp(XY) = \mexp(X^3) = 0,25 \cdot 2^3 + 0,15 \cdot (3^3 + 4^3 + 5^3) + 0.3 \cdot 6^3 = 99.2
		\]
		Dann ist  $\mcov(X,Y) = 99.2 - 4,1 \cdot 19,3 = 20.07$ und tatsächlich ergibt die explizite Rechnung denselben Wert:
		\begin{align*}
		\mcov(X,Y) &= \mathcal{E} \Big( \big(X - \mathcal{E}(X)\big)\big(Y - \mathcal{E}(Y) \big)\Big) \\
		&= 0,25 \cdot (2-4,1)(4-19,3) \\
		& \hspace{1.2em} + 0,15 \cdot (3-4,1)(9-19,3) + 0,15 \cdot (4-4.1)(16-19,3) \\
		& \hspace{1.2em} +  0,15 \cdot (5-4,1)(25-19,3)) + 0.3 \cdot (6-4,1)(36-19,3) \\
		&= 20.07
		\end{align*}
	\end{beispiel}
	
	\subsection{\mc{}n}
	
	\newcommand{\mcex}{$M = (Q, P, I)$}
	\begin{definition}[\mc]\label{def-mc}
		Eine \mc{} ist ein Tupel $(Q, P, I)$ mit den Eigenschaften
		\begin{enumerate}[(a)]
			\item $Q$ ist eine abzählbare Menge.
			\item $P : Q \times Q \to [0,1]$ mit der Eigenschaft $\forall q \in Q : \sum_{q' \in Q}{P(q,q') = 1}$.
			\item $I : Q \to [0,1]$ ist eine Wahrscheinlichkeitsverteilung auf $Q$.
		\end{enumerate}	
		Die Bilder von $P$ nennen wir auch Transitionswahrscheinlichkeiten und definieren mit welcher Wahrscheinlichkeit, nämlich $P(q,q')$ vom aktuellen Zustand $q$ in den Zustand $q'$ übergegangen wird. $I$ ist die initiale Verteilung, welche definiert, mit welcher Wahrscheinlichkeit ein Zustand als Startzustand gewählt wird.
	\end{definition}
	
	Für theoretische Betrachtungen ist es durchaus sinnvoll unendliche \mc{}n zu betrachten. Wir werden uns jedoch auf endliche beschränken, d.h. $|Q| < \infty$, sobald wir zur Berechnung von u.a. Varianzen entsprechende Gleichungssysteme herleiten. Andernfalls bestünde das Gleichungssystem aus unendlich vielen Gleichungen in unendlich vielen Variablen.
	
	\newcommand{\gpath}{Pfad}
	\newcommand{\pfin}{\mathrm{Paths}}%_{fin}
	\begin{definition}[\gpath]\label{def-path}
		Sei \mcex{} eine \mc{}. Die Menge aller endlichen \gpath e in $M$ sei definiert durch
		\begin{equation}
		\pfin(M) \coloneqq \{p \in Q^{k} \mid k \in \mathbb{N}_{>0} \land \forall 0 \leq i < k : P(p_i,p_{i+1}) > 0\}
		\end{equation}
		Wir bezeichnen mit $|p|$ die Größe des Tupels $p$, d.h. $|p| = k$ für $p \in Q^k$. Wir beginnen Indizes bei $0$.
		Die Wahrscheinlichkeit $\mathrm{\tilde{P}}(p)$ eines Pfades ist gegeben durch
		\begin{equation}
		\mathrm{\tilde{P}}(p) \coloneqq \prod_{i = 0}^{|p| - 2}{P(p_i,p_{i+1})}
		\end{equation}
		Insbesondere ist $\mathrm{\tilde{P}}(p) = 1$ für alle Pfade $p$ mit $|p| = 1$. Für Tupel $p \in Q^k$, $k \in \mathbb{N}_{>0}$, die keine Pfade sind, gilt entsprechend $\mathrm{\tilde{P}}(p) = 0$.
		
		Für $|p| = 2$ entspricht die Wahrscheinlichkeit $\mathrm{\tilde{P}}(p)$ genau der Wahrscheinlichkeit gegeben durch die Funktion $P$, nämlich der Transitionswahrscheinlichkeit aus der \mc{}. $\mathrm{\tilde{P}}$ ergibt sich als eindeutige Fortsetzung von $P$, es gilt $P = \mathrm{\tilde{P}}\vert_{Q\times Q}$, und wir schreiben daher im Folgenden einfach nur $P$.
	\end{definition}
	
	\newcommand{\reward}{Gewichtsfunktion}
	\begin{definition}[\reward]
		Sei \mcex{} eine \mc{}. Eine \reward{} auf $M$ ist eine Abbildung
		\begin{equation}
		R : Q \times Q \to \mathbb{R}\text{.}
		\end{equation} 
	\end{definition}
	Wir bezeichnen Gewichtsfunktionen typischerweise mit $R$ in Anlehnung an das englische Wort \textit{reward}.
	Offenbar gibt es analog zur Wahrscheinlichkeit $P$ eine eindeutige Fortsetzung für eine \reward{} $R$ auf Pfade, gegeben durch Aufsummierung aller Kantengewichte entlang eines solchen:
	\begin{equation}
	\mathrm{\tilde{R}} : \bigcup_{k \in \mathbb{N}_{>0}}{Q^k} \to \mathbb{R} : p \mapsto \sum_{i = 0}^{|p| - 2}{R(p_i,p_{i+1})}
	\end{equation}
	Wir schreiben im Folgenden einfach $R$.
	\begin{definition}\label{def-path-to}
		Sei \mcex{} eine \mc{}, $s \in Q$ ein beliebiger Zustand, genannt Startzustand, und $A \subseteq Q$ eine Menge von Zielzuständen. Wir bezeichnen die Menge der Pfade, welche in $s$ starten und in $A$ enden, jedoch $A$ nicht zwischenzeitlich schon erreichen, mit:
		\begin{equation}
		\pfin_{s \rightarrow A}(M) \coloneqq \{ p \in \pfin(M) \mid p_0 = s \land p_{|p|-1} \in A \land \forall i < |p| - 1 : p_i \notin A \}
		\end{equation}
		
	\end{definition}
	
	Wir ziehen nun Erkenntnisse heran, die Baier und Katoen in Kapitel 10.1 ihres Buches \textit{Principles of Model Checking} \cite{Bai08} beschreiben, speziell im Abschnitt \textit{Reachability Probabilities}.
	Setzen wir einmal voraus, dass von jedem Zustand $q\in Q$, welcher von $s$ erreichbar ist, ein Pfad in die Zielzustandsmenge $A$ existiert, das heißt
	\[
	\pfin_{s \rightarrow \{q\}}(M) \neq \emptyset  \quad \Rightarrow \quad \pfin_{q \rightarrow A}(M) \neq \emptyset \text{.}
	\]
	Unter dieser Voraussetzung beobachteten Baier und Katoen, dass es ein fast sicheres Ereignis ist, nach endlich vielen Übergängen an einem Zustand aus $A$ vorbeizukommen.
	Die endlichen Pfade, welche wir ausschließlich betrachten wollen, lassen sich als Präfixe der unendlichen Pfade $\mathrm{Paths}_{inf}(s) \coloneqq \{p \in Q^\omega \mid p(0) = s \land \forall i \in \mathbb{N}: P(p_i,p_{i+1})>0 \}$ der \mc{} auffassen.
	Man sieht leicht, dass sich ein endlicher Pfad mit der Menge an jenen unendlichen Pfaden identifizieren lässt, welche diesen als  Präfix haben.
	Eine Menge endlicher Pfade ist abzählbar, da wir die Pfade als endliche Wörter über dem Alphabet bestehend aus den Zuständen auffassen können.
	Nach dieser Beobachtung ist dann
	\[
	(\mathrm{Paths}_{s \rightarrow A}(M), P)
	\] ein \probspace{}, da die Menge aller Pfade, welche $A$ nie besuchen, ein fast unmögliches Ereignis darstellt. Die Fortsetzung von $R$ ist eine \rvar{} auf $\mathrm{Paths}_{s \rightarrow A}(M)$. 
	\begin{beispiel} \label{example-mc}
		Gegeben sei eine \mc{} \mcex{} mit
		\begin{align*}
		Q &=\{1,2,3,4,5,6\} \\
		P &=\begin{pmatrix}
		0 & 0,5 & 0,5 & 0 & 0 & 0 \\
		0 & 0 & 0 & 0 & 0 & 1 \\
		0 & 0,25 & 0 & 0,5 & 0,25 & 0 \\
		0 & 0 & 0 & 0,8 & 0,2 & 0 \\
		0 & 0 & 0 & 0 & 0 & 1 \\
		0 & 0 & 0 & 0 & 1 & 0 \\
		\end{pmatrix} \\
		I &=\begin{pmatrix} 1 & 0 & 0 & 0 & 0 & 0\end{pmatrix}^\intercal
		\end{align*}
		Wir betrachten also eine \mc{} mit dem einzigen initialen Zustand $1$. Weiterhin sei $\{5,6\}$ die betrachtete Menge an Zielzuständen, welche wir auf Pfaden erreichen wollen.	
		Eine \mc{} lässt sich immer auch als Graph auffassen: Wir beschriften alle Kanten $(i,j)$ mit Übergangswahrscheinlichkeit und Wert der Gewichtsfunktion $R$ in der Schreibweise $P(i,j) : R(i,j)$. Sei die Gewichtsfunktion $R$ implizit definiert durch folgenden Graphen:
		\begin{center}%#2
			\begin{tikzpicture}[auto,swap,scale=3]
			
			% First we draw the vertices
			\foreach \pos/\name in {{(0,0)/1}, {(1,0)/3}, {(2,0)/4}, {(0,1)/2}}
			\node[vertex] (\name) at \pos {$\name$};
			
			% First we draw the vertices
			\foreach \pos/\name in {{(1,1)/6}, {(2,1)/5}}
			\node[target] (\name) at \pos {$\name$};
			
			% Connect vertices with edges and draw weights
			\foreach \source/ \dest /\weight in {
				1/2/{\frac{1}{2}:4},
				2/6/{1:2},
				5/6/{1:3},
				6/5/{1:7}
			}
			\path[edge] (\source) to[bend left] node[weight]{$\weight$} (\dest);
			
			% Connect vertices with edges and draw weights
			\foreach \source/ \dest /\weight in {
				1/3/{\frac{1}{2}:5},
				3/4/{\frac{1}{2}:2},
				4/5/{\frac{1}{5}:3}
			}
			\path[edge] (\source) to[bend right] node{$\weight$} (\dest);
			
			% Connect vertices with edges and draw weights
			\foreach \source/ \dest /\weight in {
				3/2/{\frac{1}{4}:5},
				3/5/{\frac{1}{4}:3}
			}
			\path[edge] (\source) to node[weight]{$\weight$} (\dest);
			
			\foreach \source/ \dest /\weight in {
				4/4/{\frac{4}{5}:2}
			}
			\path[edge] (\source) to[loop right] node[weight]{$\weight$} (\dest);
			
			% Draw initial state
			\path[edge] (-0.5,0) to (1);
			
			\end{tikzpicture}
		\end{center}
		Wir wollen nun zur Veranschaulichung den Erwartungswert und die Varianz von $R$ im Wahrscheinlichkeitsraum $(\mathrm{Paths}_{1 \rightarrow \{5,6\}}(M), P)$ betrachten. Die von $5$ beziehungsweise $6$ ausgehenden Kanten samt Beschriftung sind für diesen Fall irrelevant. Die Tupel aus $Q^n$ können wir mit Wörtern über dem Alphabet $Q$ identifizieren. Wir erhalten so:
		\begin{equation*}
		\Omega \coloneqq \mathrm{Paths}_{1 \rightarrow \{5,6\}}(M) = \big\{126,1326,135\big\} \; \cup \big\{134^n5 \in Q^{n+3} \mid n\geq 1\big\} 
		\end{equation*}
		Für den Erwartungswert $\mathcal{E}(R)$ ergibt sich:
		\newcommand{\exres}{10}
		\begin{align*}
		\mathcal{E}(R) & = \sum_{p \in \Omega}{P(p) \cdot R(p)}\\
		& = \frac{1}{2}\cdot 6_{\scriptscriptstyle [126]} + \frac{1}{8}\cdot 12_{\scriptscriptstyle [1326]} + \frac{1}{8}\cdot 8_{\scriptscriptstyle [135]} + \sum_{n = 0}^{\infty}{\frac{1}{20}\cdot\left(\frac{4}{5}\right)^n \cdot (10 + 2n)}_{\scriptscriptstyle [134^n5]} \\
		& = \exres{}
		\end{align*}
		Für das Berechnen der unendlichen Summe möchten wir an dieser Stelle auf den Appendix verweisen. Mit Korollar \ref{kor-geosum} und \ref{kor-infsum} und der Zerlegung
		\begin{equation*}
		\sum_{n = 0}^{\infty}{\frac{1}{20}\cdot\left(\frac{4}{5}\right)^n \cdot (10 + 2n)}
		= \frac{1}{2}\sum_{n = 0}^{\infty}{\left(\frac{4}{5}\right)^n} + \frac{1}{10} \sum_{n = 0}^{\infty}{\left(\frac{4}{5}\right)^n \cdot n}
		\end{equation*}
		lässt sich der Wert dieser Summe, nämlich $4,5$, leicht ausrechnen.
		Als Varianz $\mvar(R)$ erhalten wir, diesmal unter Verwendung von Korollar \ref{kor-infqsum}:
		\begin{align*}
		\mvar(R) & = \mathcal{E}\big((R - \exres{})^2\big) \\
		& = \frac{1}{2}\cdot (6 - \exres)^2+ \frac{1}{8}\cdot (12-\exres)^2 + \frac{1}{8}\cdot (8-\exres)^2 + \\
		& \hspace{1.2em} + \sum_{n = 0}^{\infty}{\frac{1}{20}\cdot\left(\frac{4}{5}\right)^n \cdot (10 + 2n - \exres)^2} \\
		& = 9 + \frac{1}{5}\sum_{n = 0}^{\infty}{\left(\frac{4}{5}\right)^n \cdot n^2} \\
		& = 9 + 36 \\
		& = 45
		\end{align*}
		Es sei angemerkt, dass die Zufallsvariable $X \coloneqq (R-\exres)^2$, per Definition
		\[
		(R-\exres)^2 : \mathrm{Paths}_{1 \rightarrow \{5,6\}}(M) \to  \mathbb{R} : p \mapsto (R(p) - \exres)^2\text{,}
		\]
		im Allgemeinen nicht die Eigenschaft der eindeutigen Fortsetzung von Kanten zu Pfaden erfüllt:
		\[
		\forall n \in \mathbb{N}, p \in Q^n : n>1 \Rightarrow X(p) = \sum_{i=0}^{n-2}{X(p(i),p(i+1))}
		\]
		Beispielsweise verletzt $X(1,3) + X(3,2) = 25 + 25 \neq 0 = X(132)$ diese Eigenschaft. Während im Graphen eingezeichnete Kantengewichte immer zu einer \rvar{} auf der Menge der Pfade fortgesetzt werden können, können wir nicht alle \rvar{}n auf Kantengewichte zurückführen, insbesondere X also nicht durch Kantengewichte im Graphen einzeichnen.
		
		
		An diesem Beispiel sehen wir, dass die explizite Berechnung von Varianzen bereits kompliziert werden kann, wenn nur ein Kreis trivialer Länge im Graphen der \mc{} enthalten ist. Stellen wir uns einen Graphen vor, in dem Knoten $a$ und $b$ existieren sowie Kanten $(a,a), (a,b), (b,a)$. Dann gibt es nicht mehr nur eine Möglichkeit, einen Kreis zu finden, sondern unendlich viele mit beispielsweise $(a)^\omega, (ab)^\omega, (aab)^\omega, (aaab)^\omega, (aaaab)^\omega,\dots$\;.
	\end{beispiel}
	
	\begin{definition}\label{def-pmod}
		Sei \mcex{} eine endliche \mc, $A\subseteq Q$ eine Zielmenge. Dann bezeichnen wir mit $P_{\rightarrow A}$ die folgende Matrix:
		\begin{equation}
		P_{\rightarrow A} : Q^2 \to [0,1] : \begin{cases}
		P(s,t) & \text{falls } s\notin A\\
		0 & \text{falls } s\in A
		\end{cases}
		\end{equation}
	\end{definition}
	
	Diese Modifikation entspricht dem Entfernen von allen ausgehenden Kanten von Zielzuständen $a\in A$.
	
	\begin{satz}\label{th-unique}
		Sei \mcex{} eine endliche \mc, $A\subseteq Q$ eine Zielmenge, die von jedem Knoten aus erreichbar ist ($\forall s\in Q: \mathrm{Paths}_{s\rightarrow A}(M)\neq\emptyset$). 
		Dann ist die Matrix $D \coloneqq P_{\rightarrow A} - \mathbb{1}$ invertierbar.
	\end{satz}
	\begin{beweis}
		%Sei $R: Q^2 \to \mathbb{R} : (s,t) \mapsto 0$ die triviale \reward{} auf $M$. 
		Aus der linearen Algebra ist bekannt, dass $D$ genau dann invertierbar ist, wenn $Dx = 0$ nur die Lösung $x=0$ hat.
		Nehmen wir also an, $x \in \mathbb{R}^Q$ mit $x\neq 0$ erfüllt $Dx = 0$. O.B.d.A. habe $x$ einen positiven Eintrag. (Hat $x$ keinen positiven Eintrag, so hat es einen negativen Eintrag. Dann finden wir mit $-x$ eine Lösung mit einem positivem Eintrag, da $D(-x) = - Dx = 0$.)
		%Dann gilt auch $\forall k \in \mathbb{R}: D (kx) = 0$. Wählen wir ein geeignetes $k$, so erhalten wir eine Lösung $z$ mit einem positiven Eintrag, d.h. $Dz = 0$ und $\exists q \in Q : z_q > 0$.
		Sei $E \subseteq Q$ die Indexmenge aller maximalen Einträge von $x$, also $e \in E :\Leftrightarrow \forall q \in Q : x_q \leq x_e$.
		Sei $s\in A$. Dann folgt aus $(Dx)_s = 0$, dass $\sum_{q\in Q}{(P_{\rightarrow A}- \mathbb{1})_{(s,q)} \cdot x_q} = -\sum_{q\in Q}{\mathbb{1}_{(s,q)} \cdot x_q} = 0$  und damit $x_s = 0$.
		
		Sei nun $s \in E$ frei gewählt. Dann ist $x_s > 0$ und daher $s\notin A$. Aus $(Dx)_s = 0$ folgt dann $\sum_{q\in Q}{(P_{\rightarrow A}- \mathbb{1})_{(s,q)} \cdot x_q} = 0$. Damit bekommen wir $\sum_{q\in Q}{P_{(s,q)} \cdot x_q} = x_s$. Da nach Definition \ref{def-mc} die Abbildung $q \mapsto P(s,q)$ eine Wahrscheinlichkeitsverteilung auf $Q$ ist, folgt schließlich $\forall q\in Q: (P(s,q) > 0 \Rightarrow x_q = x_s)$. Schließlich kann die gewichtete Summe nur genau dann den maximalen Wert $x_s$ annehmen, wenn alle zu gewichtenden Summanden bereits maximal sind. D.h. für jeden Nachfolgeknoten $t\in Q$ von $s\in E$ in der \mc{} $M$ liegt $t$ selbst wieder in $E$. Da ein Knoten $a\in A$ von $s$ aus erreichbar ist, gilt auch $a\in E$. Aber dann wäre $a\notin A$. Wir erhalten einen Widerspruch.
	\end{beweis}
	
	
	\section{Formale Herleitung eines Algorithmus}
	
	Ziel ist es nun, Algorithmen zu beschreiben und zu analysieren, welche bei gegebener \mc{} $M$, gegebenem Startzustand $s$ und gegebener Zielzustandsmenge $A$ die Varianz bzw. Kovarianz berechnen. Wir beschränken uns dabei auf den Fall, dass $A$ von jedem Zustand in $M$ aus erreichbar ist.
	
	\begin{definition}
		Sei $Q$ eine Menge, $n\in \mathbb{N}, n>2$ und $p \in Q^n$, in folgenden Betrachtungen als Pfad aufgefasst. Dann bezeichnen wir mit $p_{\leftarrow 1}$ den Teilpfad von $p = (p_0, \dots, p_{n-1})$ ohne den ersten Knoten $p_0$:
		\begin{equation}
		p_{\leftarrow 1} \coloneqq (p_1,p_2, \dots p_{n-1})
		\end{equation}
	\end{definition}
	
	\subsection{Berechnung von Erwartungswerten}
	
	Seien \mcex{} eine \mc{}, $s \in Q$, $\emptyset \neq A \subseteq Q$ und sei von jedem Zustand $q\in Q$ ein Zustand in $A$ erreichbar. Sei $R$ eine \reward{} auf $M$. Wir betrachten im \probspacen{} $(\mathrm{Paths}_{s \rightarrow A}(M), P)$ den Erwartungswert $\mathcal{E}_{(\mathrm{Paths}_{s \rightarrow A}(M), P)}(R)$, im Folgenden kurz $\mathcal{E}_{s}(R)$:
	
	\begin{equation}
	\mathcal{E}_{s}(R) = \sum_{p \in \mathrm{Paths}_{s \rightarrow A}(M)}{P(p) \cdot R(p)} 
	\end{equation}
	Falls $s \in A$, gilt $|\mathrm{Paths}_{s \rightarrow A}(M)| = 1$ mit dem einzigen enthaltenen Pfad $p = (s)$. Dann sind nach Definition $P(p) = 1$ und $R(p) = 0$. Damit erhalten wir:
	
	\begin{align}
	\mathcal{E}_{s}(R) = 0 && \text{(falls $s \in A$)}\label{expect_trivial}
	\end{align}
	
	Falls $s \notin A$, besteht jeder Pfad $p \in \mathrm{Paths}_{s \rightarrow A}(M)$ aus mehr als einem Knoten, und wir erhalten:
	\begin{align}
	\mathcal{E}_{s}(R) & = \sum_{p \in \mathrm{Paths}_{s \rightarrow A}(M)}{P(s,p_1) \cdot P(p_{\leftarrow 1}) \cdot (R(s,p_1) + R(p_{\leftarrow 1}))} \\
	& = \sum_{t \in Q}{ P(s,t) \cdot \sum_{p' \in \mathrm{Paths}_{t \rightarrow A}(M)}{ P(p') \cdot (R(s,t) + R(p')) } } \\
	& = \sum_{t \in Q}{ P(s,t) \cdot \left(R(s,t) + \sum_{p' \in \mathrm{Paths}_{t \rightarrow A}(M)}{ P(p') \cdot R(p') } \right) } \\
	& = \sum_{t \in Q}{ P(s,t) \cdot \big(R(s,t) + \mathcal{E}_{t}(R) \big) } \label{expect_recursive}
	\end{align}
	Die Gleichungen \ref{expect_trivial} und \ref{expect_recursive} geben uns ein System von $|Q|$ linearen Gleichungen in $|Q|$ Variablen:
	\begin{align}
	\begin{aligned}
	\mu_{s} & = & 0 && \text{(falls $s \in A$)} \\
	\mu_{s} & = & \sum_{t \in Q}{ P(s,t) \cdot \left(R(s,t) + \mu_{t} \right) } && \text{(falls $s \notin A$)}
	\end{aligned}\label{les-exp}
	\end{align}
	
	Mit Definition \ref{def-pmod} ist dieses Gleichungssystem äquivalent zu
	\begin{equation}
	\mu_s = \sum_{t \in Q}{ P_{\rightarrow A}(s,t) \cdot \left(R(s,t) + \mu_{t} \right) }\text{,}
	\end{equation}
	was sich in Matrixschreibweise mit $\mu = (\mu_s)_{s \in Q }$ ausdrücken lässt als:
	\begin{equation}
	\mu = \left(\sum_{t \in Q}{ P_{\rightarrow A}(s,t) \cdot R(s,t) }\right)_{s \in Q} + P_{\rightarrow A} \cdot \mu 
	\end{equation}
	\begin{satz}[\expect{}e in \mc{}n]\label{th-exp}
		Seien \mcex{} eine \mc{} und $\emptyset \neq A\subseteq Q$ und sei von jedem Zustand $q\in Q$ ein Zustand in $A$ erreichbar. Dann ist der Vektor $\mu = (\mu_s)_{s \in Q }$ der Erwartungswerte akkumulierter Kantengewichte einer Gewichtsfunktion $R$ auf Pfaden von $s$ in die Menge $A$ die eindeutige Lösung des Gleichungssystems	
		\begin{equation}
		(P_{\rightarrow A} - \mathbb{1}) \mu = - \left(\sum_{t \in Q}{ P_{\rightarrow A}(s,t) \cdot R(s,t) }\right)_{s \in Q}\text{.}\label{les-exp-mat}
		\end{equation}
	\end{satz}
	\begin{beweis}
		Wir stellen fest, der Vektor $(\mathcal{E}_{s}(R))_{s \in Q}$ ist die einzige Lösung des Gleichungssystems (\ref{les-exp-mat}):
		Zum einen ist gemäß unserer Herleitung der Vektor $(\mathcal{E}_{s}(R))_{s \in Q}$ eine Lösung dieses Gleichungssystems. Zum anderen ist $P_{\rightarrow A} - \mathbb{1}$ nach Satz \ref{th-unique} invertierbar. Somit ist die Lösung eindeutig.
	\end{beweis}
	
	Mit dem Gleichungssystem (\ref{les-exp-mat}) und Standardalgorithmen zum Lösen linearer Gleichungssysteme erhalten wir unmittelbar einen Algorithmus zur Berechnung der Erwartungswerte $\mexp_q(R)$ für $q \in Q$.
	
	\begin{beispiel}\label{example-mc-exp-les}
		Wir wollen nun Beispiel \ref{example-mc} fortsetzen und den Erwartungswert mittels linearem Gleichungssystem lösen. Seien also $M$ und $R$ wie bisher definiert.
		Dann ergibt sich die Matrix $P_{\rightarrow \{5,6\}} - \mathbb{1}$ als
		\begin{equation*}
		P_{\rightarrow \{5,6\}} - \mathbb{1} = \begin{pmatrix}
		-1 & 0,5 & 0,5 & 0 & 0 & 0 \\
		0 & -1 & 0 & 0 & 0 & 1 \\
		0 & 0,25 & -1 & 0,5 & 0,25 & 0 \\
		0 & 0 & 0 & -0,2 & 0,2 & 0 \\
		0 & 0 & 0 & 0 & -1 & 0 \\
		0 & 0 & 0 & 0 & 0 & -1 \\
		\end{pmatrix}\text{.}
		\end{equation*}
		Als Inverse von $P_{\rightarrow \{5,6\}} - \mathbb{1}$ erhalten wir
		\begin{equation*}
		(P_{\rightarrow \{5,6\}} - \mathbb{1})^{-1} = \begin{pmatrix}
		-1 & -\frac{5}{8} & -\frac{1}{2} & -\frac{5}{4} & -\frac{3}{8} & -\frac{5}{8} \\
		0 & -1 & 0 & 0 & 0 & -1 \\
		0 & -\frac{1}{4} & -1 & -\frac{5}{2} & -\frac{3}{4} & -\frac{1}{4} \\
		0 & 0 & 0 & -5 & -1 & 0 \\
		0 & 0 & 0 & 0 & -1 & 0 \\
		0 & 0 & 0 & 0 & 0 & -1 \\
		\end{pmatrix}\text{.}
		\end{equation*}
		Außerdem berechnen wir wie in Satz \ref{th-exp} beschrieben einen Vektor:
		\begin{align*}
		b &\coloneqq - \left(\sum_{t \in Q}{ P_{\rightarrow \{5,6\}}(s,t) \cdot R(s,t) }\right)_{s \in Q} \\
		&= \begin{pmatrix} -\frac{9}{2} & -2 & -3 & -\frac{11}{5} & 0 & 0 \end{pmatrix}^\intercal
		\end{align*}
		Wir erhalten in der Folge für das Produkt $\mu \coloneqq (P_{\rightarrow \{5,6\}} - \mathbb{1})^{-1}b$ den Vektor
		\begin{equation*}
		\mu = \begin{pmatrix} 10 & 2 & 9 & 11 & 0 & 0 \end{pmatrix}^\intercal\text{.}
		\end{equation*}
		Auf diese Weise haben wir den Erwartungswert aufsummierter Kantengewichte entlang von Pfaden von Knoten $1$ bis zum Erreichen eines Knotens in $\{5,6\}$, nämlich $10$ berechnet, kennen jedoch gleichzeitig schon alle Erwartungswerte für den Start in einem anderen Knoten.
	\end{beispiel}
	
	\subsection{Berechnung von Varianzen}
	
	Die Betrachtung zu \cov{}en sind weitestgehend analog zu den Betrachtungen zu \var{}en, welche wir hier darstellen wollen. Man kann Satz \ref{th-var}, welchen wir in diesem Abschnitt zeigen wollen, auch als Korollar des Satzes \ref{th-cov} auffassen, welcher im nachfolgenden Abschnitt vorgestellt wird. Man kann also bei Bedarf diesen Abschnitt überspringen. Dennoch wollten wir der Anschaulichkeit wegen den etwas einfacheren Fall der \var{} voranstellen.
	
	Seien \mcex{} eine \mc{}, $s \in Q$, $\emptyset \neq A \subseteq Q$ und  sei von jedem Zustand $q\in Q$ ein Zustand in $A$ erreichbar. Sei $R$ eine  \reward{} auf $M$. Wir betrachten in $(\mathrm{Paths}_{s \rightarrow A}(M), P)$ die Varianz $\mvar_{(\mathrm{Paths}_{s \rightarrow A}(M), P)}(R)$:
	\begin{equation}
	\mvar_{(\mathrm{Paths}_{s \rightarrow A}(M), P)}(R) = \mathcal{E}_{(\mathrm{Paths}_{s \rightarrow A}(M), P)}\left(\left(R - \mathcal{E}_{(\mathrm{Paths}_{s \rightarrow A}(M), P)} (R)\right)^{2}\right) 
	\end{equation}
	Erneut nutzen wir die Kurzschreibweise $\mvar_{s}(R) \coloneqq \mvar_{(\mathrm{Paths}_{s \rightarrow A}(M), P)}(R)$ und erhalten:
	\begin{equation}
	\mvar_{s}(R) = \mathcal{E}_{s}\left(\left(R - \mathcal{E}_{s} (R)\right)^{2}\right)
	\end{equation}
	Falls $s \in A$, gilt $|\mathrm{Paths}_{s \rightarrow A}(M)| = 1$ mit dem einzigen enthaltenen Pfad $p = (s)$. Dann sind nach Definition $P(p) = 1$ und $R(p) = 0$ und der Erwartungswert beträgt $\mathcal{E}_{s}(R) = 0$, wie wir bereits im vorangegangenen Abschnitt gesehen haben. Damit erhalten wir:
	
	\begin{align}
	\mvar_{s}(R) = 0 && \text{(falls $s \in A$)}\label{var_trivial}
	\end{align}
	
	Falls $s \notin A$, besteht jeder Pfad $p \in \mathrm{Paths}_{s \rightarrow A}(M)$ aus mehr als einem Knoten, und wir erhalten:
	\begin{align}
	\mvar_{s}(R) & = \sum_{p \in \mathrm{Paths}_{s \rightarrow A}(M)}{P(p) \cdot \left(R(p) - \mathcal{E}_{s}(R)\right)^2} \\
	& = \sum_{t \in Q}{ P(s,t) \cdot \sum_{p' \in \mathrm{Paths}_{t \rightarrow A}(M)}{ P(p') \cdot \left(R(s,t) + R(p') - \mathcal{E}_{s}(R)\right)^2 } } \\
	& = \sum_{t \in Q}{ P(s,t) \cdot \mathcal{E}_{t}\left(\left(R + R(s,t) - \mathcal{E}_{s}(R)\right)^2\right) } \\
	& = \sum_{t \in Q}{ P(s,t) \cdot \bigg(\mvar_{t}(R) + \Big(\mathcal{E}_{t}\big(R + R(s,t) - \mathcal{E}_{s}(R)\big)\Big)^2\bigg) } \label{pre-var_recursive} \\
	& = \sum_{t \in Q}{ P(s,t) \cdot \Big(\mvar_{t}(R) + \big(\mathcal{E}_{t}(R) + R(s,t) - \mathcal{E}_{s}(R)\big)^2\Big) } \label{var_recursive}
	\end{align}
	
	Gleichung (\ref{pre-var_recursive}) erhalten wir durch Anwendung von Korollar \ref{kor-var-exp} und Gleichung (\ref{var_recursive}) durch die Linearität des Erwartungswertes nach Lemma \ref{lem-explin}.
	Die Gleichungen (\ref{var_trivial}) und (\ref{var_recursive}) geben uns analog zum Abschnitt über die \expect{}e ein System von $|Q|$ linearen Gleichungen in $|Q|$ Variablen:
	\begin{align}
	\begin{aligned}
	\nu_s & = & 0 && \text{(falls $s \in A$)}\\
	\nu_s & = & \sum_{t \in Q}{ P(s,t) \cdot \Big(\nu_t + \big(\mathcal{E}_{t}(R) + R(s,t) - \mathcal{E}_{s}(R)\big)^2\Big) } && \text{(falls $s \notin A$)}
	\end{aligned}\label{les-var}
	\end{align}
	
	%Sei die \reward{} $S$ auf definiert durch $S: Q^2 \to \mathbb{R}_+ : (s,t) \mapsto \big(\mathcal{E}_{t}(R) + R(s,t) - \mathcal{E}_{s}(R)\big)^2$ und sei $\nu = (\nu_s)_{s\in Q}$. Analog zu Gleichung (\ref{les-exp-mat}) bekommen wir durch Anwendung von Definition \ref{def-pmod} das äquivalente Gleichungssystem
	\begin{satz}[\var{}en in \mc{}n] \label{th-var}
		Sei \mcex{} eine \mc{}. Sei $\emptyset \neq A\subseteq Q$ und sei von jedem Zustand $q\in Q$ ein Zustand in $A$ erreichbar. Dann ist der Vektor $\nu = (\nu_s)_{s \in Q } = (\mvar_{s}(R))_{s \in Q }$ der Varianzen akkumulierter Kantengewichte einer Gewichtsfunktion $R$ auf Pfaden von $s$ in die Menge $A$ die eindeutige Lösung des Gleichungssystems	
		\begin{equation}
		(P_{\rightarrow A} - \mathbb{1}) \nu = - \left(\sum_{t \in Q}{ P_{\rightarrow A}(s,t) \cdot S(s,t) }\right)_{s \in Q}\text{,}\label{les-var-mat}
		\end{equation}
		wobei $S$ die Gewichtsfunktion definiert durch $S: Q^2 \to \mathbb{R}_+ : (s,t) \mapsto \big(\mathcal{E}_{t}(R) + R(s,t) - \mathcal{E}_{s}(R)\big)^2$ ist.
	\end{satz}
	\begin{beweis}
		Durch Anwendung von Definition \ref{def-pmod} auf Gleichung (\ref{les-var}) erhalten wir direkt dieses Gleichungssystem. Daher ist der Vektor der Varianzen tatsächlich eine Lösung. Nach Satz \ref{th-unique} ist die Lösung eindeutig.
	\end{beweis}
	
	Vergleicht man die Gleichungssysteme (\ref{les-exp-mat}) und (\ref{les-var-mat}), dann bekommen wir:
	\begin{equation}
	\forall q \in Q : \mvar_q(R) = \mathcal{E}_q(S)
	\end{equation}
	Damit haben wir die Berechnung der Varianzen gleichzeitig auf die Berechnung von Erwartungswerten zurückgeführt. Um die \var{}en $(\mvar_q(R))_{q\in Q}$ in einer \mc{} zu berechnen, genügt es die \expect{}e $(\mathcal{E}_q(R))_{q\in Q}$ im ersten Schritt und $(\mathcal{E}_q(S))_{q\in Q}$ im zweiten Schritt zu berechnen. 
	Berechnen wir zuerst die inverse Matrix $(P_{\rightarrow A} - \mathbb{1})^{-1}$, dann müssen wir zum Lösen beider Gleichungssysteme jeweils nur eine Matrixmultiplikation durchführen.
	
	\begin{beispiel}\label{example-mc-var-les}
		Wir wollen erneut an unsere bisherige konkrete \mc{} anknüpfen und setzen die Beispiele \ref{example-mc} und \ref{example-mc-exp-les} fort.
		Zunächst berechnen wir die konkrete Gewichtsfunktion $S$, definiert durch
		\[
		S: Q^2 \to \mathbb{R}_+ : (s,t) \mapsto \big(\mathcal{E}_{t}(R) + R(s,t) - \mathcal{E}_{s}(R)\big)^2\text{.}
		\]
		Diese lässt sich als Matrix notieren:
		\begin{equation*}
		S = \begin{pmatrix}
		0 & 16 & 16 & 1 & 100 & 100 \\
		64 & 0 & 49 & 81 & 4 & 0 \\
		1 & 4 & 0 & 16 & 36 & 81 \\
		1 & 81 & 4 & 4 & 64 & 121 \\
		100 & 4 & 81 & 121 & 0 & 9 \\
		100 & 4 & 81 & 121 & 49 & 0 \\
		\end{pmatrix}
		\end{equation*}
		Tatsächlich sind für die weitere Berechnung bei Weitem nicht alle Einträge der Matrix relevant, sondern nur die für solche Paare $(s,t)$, die Kanten in $P_{\rightarrow \{5,6\}}$ darstellen, d.h. $(s,t) \in \mathrm{supp}(P_{\rightarrow \{5,6\}})$. Es reicht also eine Matrix $\tilde{S}$ mit
		\begin{equation*}
		\tilde{S} = \begin{pmatrix}
		0 & 16 & 16 & 0 & 0 & 0 \\
		0 & 0 & 0 & 0 & 0 & 0 \\
		0 & 4 & 0 & 16 & 36 & 0 \\
		0 & 0 & 0 & 4 & 64 & 0 \\
		0 & 0 & 0 & 0 & 0 & 0 \\
		0 & 0 & 0 & 0 & 0 & 0 \\
		\end{pmatrix}\text{.}
		\end{equation*}
		Dann berechnen wir analog zu Beispiel \ref{example-mc-exp-les} nach Satz \ref{th-var} einen Vektor:
		\begin{align*}
		c &\coloneqq - \left(\sum_{t \in Q}{ P_{\rightarrow \{5,6\}}(s,t) \cdot S(s,t) }\right)_{s \in Q} \\
		&= - \left(\sum_{t \in Q}{ P_{\rightarrow \{5,6\}}(s,t) \cdot \tilde{S}(s,t) }\right)_{s \in Q} \\
		&= \begin{pmatrix} -16 & 0 & -18 & -16 & 0 & 0 \end{pmatrix}^\intercal
		\end{align*}
		Wir erhalten in der Folge für das Produkt $\nu \coloneqq (P_{\rightarrow \{5,6\}} - \mathbb{1})^{-1}c$ den Vektor
		\begin{equation*}
		\nu = \begin{pmatrix} 45 & 0 & 58 & 80 & 0 & 0 \end{pmatrix}^\intercal\text{.}
		\end{equation*}
		Im ersten Eintrag des Vektors sehen wir dieselbe Varianz, welche wir bereits in Beispiel \ref{example-mc} ermittelt haben. Die \var{} für den Startknoten $2$ ist $0$ in Übereinstimmung mit der Eigenschaft, dass es von dort aus nur einen eindeutigen Pfad gibt und somit die Abweichung vom \expect{} in jedem Fall $0$ beträgt. Die Varianzen für Knoten $3$ und $4$ sind entsprechend $\mvar_3(R) = 58$ bzw. $\mvar_4(R) = 80$.
		
	\end{beispiel}
	
	\subsection{Berechnung von Kovarianzen}
	
	Seien wieder \mcex{} eine \mc{}, $s \in Q$, $\emptyset \neq A \subseteq Q$ und sei von jedem Zustand $q\in Q$ ein Zustand in $A$ erreichbar. Seien nun $X, Y$ \reward{}en auf $M$. Wir betrachten im \probspacen{} $(\mathrm{Paths}_{s \rightarrow A}(M), P)$ die \cov{} $\mcov_s(X,Y) \coloneqq \mcov_{(\mathrm{Paths}_{s \rightarrow A}(M), P)}(X,Y)$:
	
	\begin{equation}
	\mcov_{s}(X,Y) = \mathcal{E}_{s}\big(\left(X - \mathcal{E}_{s} (X)\right)\left(Y - \mathcal{E}_{s} (Y)\right)\big)
	\end{equation}
	Falls $s \in A$, gilt $|\mathrm{Paths}_{s \rightarrow A}(M)| = 1$ mit dem einzigen enthaltenen Pfad $p = (s)$. Dann sind nach Definition $P(p) = 1$ und $R(p) = 0$ und die \expect{}e betragen $\mathcal{E}_{s}(X) = \mathcal{E}_{s}(Y) = 0$. Damit erhalten wir:
	
	\begin{align}
	\mcov_{s}(X,Y) = 0 && \text{(falls $s \in A$)}\label{cov_trivial}
	\end{align}
	
	Falls $s \notin A$, besteht jeder Pfad $p \in \mathrm{Paths}_{s \rightarrow A}(M)$ aus mehr als einem Knoten und wir erhalten:
	
	\begin{align}
	\mcov_{s}(X,Y) & = \sum_{p \in \mathrm{Paths}_{s \rightarrow A}(M)}{P(p) \cdot \big(\left(X(p) - \mathcal{E}_{s} (X)\right)\left(Y(p) - \mathcal{E}_{s} (Y)\right)\big)} \\
	& = \sum_{t \in Q}{ P(s,t) \sum_{p' \in \mathrm{Paths}_{t \rightarrow A}(M)}{ P(p') \cdot \begin{pmatrix}
			(X(s,t) + X(p') - \mathcal{E}_{s}(X)) \;\cdot \\
			\cdot \;(Y(s,t) + Y(p') - \mathcal{E}_{s}(Y)) \\
			\end{pmatrix}}} \label{step1} \\
	& = \sum_{t \in Q}{ P(s,t) \cdot \mathcal{E}_{t}\begin{pmatrix}
		\big(X + X(s,t) - \mathcal{E}_{s}(X)\big)\;\cdot \\
		\cdot\;\big(Y + Y(s,t) - \mathcal{E}_{s}(Y)\big) \\
		\end{pmatrix}} \label{step2}\\
	& = \sum_{t \in Q}P(s,t) \cdot \begin{pmatrix}
	\mathcal{E}_{t}(XY) + \mathcal{E}_{t}(X)(Y(s,t) - \mathcal{E}_{s}(Y)) \; + \\
	+\; \mathcal{E}_{t}(Y)(X(s,t) - \mathcal{E}_{s}(X)) \; + \\
	+\; (X(s,t) - \mathcal{E}_{s}(X))(Y(s,t) - \mathcal{E}_{s}(Y)) \\
	\end{pmatrix} \label{step3} \\
	& = \sum_{t \in Q}P(s,t) \cdot \begin{pmatrix}
	\mathcal{E}_{t}(XY) - \mathcal{E}_{t}(X)\mathcal{E}_{t}(Y)\;+ \\
	+\;\mathcal{E}_{t}(X)\mathcal{E}_{t}(Y) + \mathcal{E}_{t}(X)(Y(s,t) - \mathcal{E}_{s}(Y))\;+ \\
	+\; \mathcal{E}_{t}(Y)(X(s,t) - \mathcal{E}_{s}(X))\;+ \\
	+\; (X(s,t) - \mathcal{E}_{s}(X))(Y(s,t) - \mathcal{E}_{s}(Y)) \\
	\end{pmatrix} \label{step4}\\
	& = \sum_{t \in Q}P(s,t) \cdot \Bigg( \mcov_{t}(X,Y) + \begin{pmatrix}
	\mathcal{E}_{t}(X) + X(s,t) - \mathcal{E}_{s}(X)\big)\;\cdot \\
	\cdot\;\big(\mathcal{E}_{t}(Y) + Y(s,t) - \mathcal{E}_{s}(Y)\big) \\
	\end{pmatrix}\Bigg) \label{cov_recursive}
	\end{align}
	Gleichung (\ref{step1}) erhalten wir durch Partitionierung der Pfade nach der ersten Transition.
	Mit Definition \ref{def-expect} erhalten wir Gleichung (\ref{step2}).
	Gleichung (\ref{step3}) erhalten wir durch partielles Ausmultiplizieren und anschließendes Anwenden der Linearität (Lemma \ref{lem-explin}). Um Gleichung (\ref{step4}) zu erhalten, addieren wir an geeigneter Stelle $- \mathcal{E}_{t}(X)\mathcal{E}_{t}(Y) + \mathcal{E}_{t}(X)\mathcal{E}_{t}(Y) = 0$. Danach können wir die ersten zwei Summanden der großen Klammer nach Lemma \ref{lemma-cov-exp} als \cov{} auffassen sowie den Rest durch geschicktes Ausklammern kompakt darstellen und erhalten Gleichung (\ref{cov_recursive}).
	
	Sei die \reward{} $S$ auf $M$ definiert als
	\begin{equation}
	S: Q^2 \to \mathbb{R} : (s,t) \mapsto \big(\mathcal{E}_{t}(X) + X(s,t) - \mathcal{E}_{s}(X)\big)\big(\mathcal{E}_{t}(Y) + Y(s,t) - \mathcal{E}_{s}(Y)\big)\text{.} \label{eq-cov-rew}
	\end{equation}
	Die Gleichungen (\ref{cov_trivial}) und (\ref{cov_recursive}) geben uns ähnlich den Abschnitten über die \expect{}e bzw. \cov{}en ein System von $|Q|$ linearen Gleichungen in $|Q|$ Variablen:
	\begin{equation}
	\begin{aligned}
	c_s & = & 0 && \text{(falls $s \in A$)}\\
	c_s & = & \sum_{t \in Q}P(s,t) \cdot \big(c_t + S(s,t)\big) && \text{(falls $s \notin A$)} 
	\end{aligned} \label{les-cov}
	\end{equation}
	\begin{satz}[\cov{}en in \mc{}n] \label{th-cov}
		\quad Seien \mcex{} eine \mc{} und $\emptyset \neq A\subseteq Q$. Sei von jedem Zustand $q\in Q$ ein Zustand in $A$ erreichbar. Dann ist der Vektor $c = (c_s)_{s \in Q } = (\mcov_{s}(X,Y))_{s \in Q }$ der \cov{}en akkumulierter Kantengewichte bezüglich Gewichtsfunktionen $X,Y$ auf Pfaden von $s$ in die Menge $A$ die eindeutige Lösung des Gleichungssystems
		\begin{equation}
		(P_{\rightarrow A} - \mathbb{1}) c = - \left(\sum_{t \in Q}{ P_{\rightarrow A}(s,t) \cdot S(s,t) }\right)_{s \in Q} \text{,}\label{les-cov-mat}
		\end{equation}
		wobei $S$ die in Gleichung (\ref{eq-cov-rew}) definierte aus $X$ und $Y$ abgeleitete Kantengewichtsfunktion bezeichnet.
	\end{satz}
	\begin{beweis}
		Analog zu vorherigen Abschnitten erhalten wir durch Anwendung von Definition \ref{def-pmod} auf das Gleichungssystem (\ref{les-cov}), dass der Vektor der \cov{}en in der Tat Lösung des dargestellten Gleichungssystems ist. Wieder besitzt das Gleichungssystem eine eindeutige Lösung nach Satz \ref{th-unique}.
	\end{beweis}
	
	Vergleicht man die Gleichungssysteme (\ref{les-exp-mat}) und (\ref{les-cov-mat}), dann bekommen wir:
	\begin{equation}
	\forall q \in Q : \mcov_q(X,Y) = \mathcal{E}_q(S)
	\end{equation}
	Das entspricht dem Zurückführen der Berechnung von Kovarianzen auf die Berechnung von Erwartungswerten. Um die \cov{}en $(\mcov_q(X,Y))_{q\in Q}$ in einer \mc{} zu berechnen, genügt es die \expect{}e $(\mathcal{E}_q(X))_{q\in Q}$ sowie $(\mathcal{E}_q(Y))_{q\in Q}$ im ersten Schritt und $(\mathcal{E}_q(S))_{q\in Q}$ im zweiten Schritt zu berechnen. Es ist wie bei den Varianzen möglich, zuerst die inverse Matrix $(P_{\rightarrow A} - \mathbb{1})^{-1}$ zu berechnen, um danach nur noch Matrixmultiplikationen ausführen zu müssen.
	
	\subsection{Die initiale Verteilung}
	
	Wir haben bisher nur über die Erwartungswerte und (Ko-) Varianzen für den Start in einem speziellen Zustand gesprochen. Wenn es darum geht, diese Kenngrößen einer \mc{} \mcex{} insgesamt zu berechnen, muss aber auch die initiale Zufallsverteilung $I : Q \to [0,1]$ in Betracht gezogen werden. Der Erwartungswert der \mc{} ist als die gewichtete Summe $\mu = \sum_{q\in Q}{I(q) \cdot \mu_q}$ definiert. Um nicht Erwartungswert und (Ko-) Varianz der gesamten \mc{} auf diese Weise gesondert betrachten zu müssen, können wir uns dem Trick bedienen, mit $Q' \coloneqq Q \cup \{q_{start}\}$ einen neuen Zustand $q_{start} \notin Q$ hinzuzufügen. Mit $I'$ und $P'$ definiert als
	\begin{multicols}{2}
		\noindent
		\begin{equation*}
		\hspace{-1ex}I'(q) \coloneqq \begin{cases}
		1 & q = q_{start}\\
		0 & q \neq q_{start}
		
		\end{cases}
		\end{equation*}
		\begin{equation*}
		\hspace{-1ex}P'(s, t)  \coloneqq \begin{cases}
		I(t) & s = q_{start} \land t \in Q \\
		0 & t = q_{start}\\
		P(s,t) & s,t \in Q
		\end{cases}
		\end{equation*}
	\end{multicols}
	erhalten wir eine neue \mc{} $M'=(Q',P',I')$, wobei \expect{} und (Ko-) Varianz von $M$ und $M'$ mit den entsprechenden Werten von $q_{start}$ übereinstimmen.
	
	Wir wollen uns deshalb mit der zustandsweisen Betrachtung der Größen begnügen. Ferner können wir die genannten Größen neben alternativen Mög\-lich\-keit\-en mithilfe genau dieser Betrachtung definieren. Seien dazu $R, S$ Gewichtsfunktionen und A eine Zielzustandsmenge. Wir erweitern $R$ entsprechend so, dass $R(s,t) = 0$, wenn $q_{start} \in \{s,t\}$ und analog für $S$. Wir definieren \expect{}, \var{} und \cov{} für $M$ als:
	\begin{align*}
	\mathcal{E}(R) \coloneqq \;& \mathcal{E}_{(\mathrm{Paths}_{q_{start} \rightarrow A}(M'), P')}(R) \\
	\mvar(R) \coloneqq \;& \mvar_{(\mathrm{Paths}_{q_{start} \rightarrow A}(M'), P')}(R) \\
	\mcov(R,S) \coloneqq \;& \mcov_{(\mathrm{Paths}_{q_{start} \rightarrow A}(M'), P')}(R,S) \\
	\end{align*}
	
\end{document}